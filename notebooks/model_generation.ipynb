{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_spaces(df):\n",
    "    old_column_names = df.columns.tolist()\n",
    "    new_column_names =[]\n",
    "    for col in old_column_names:\n",
    "        col = col.replace(' ', '')\n",
    "        new_column_names.append(col)\n",
    "    df.columns= new_column_names\n",
    "    return df \n",
    "\n",
    "def separate_train_test(df,testsize =0.2):\n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = testsize,random_state=0)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan_approval_dataset.csv')\n",
    "df = clean_column_spaces(df)\n",
    "X_train,X_test,y_train,y_test = separate_train_test(df)\n",
    "\n",
    "le_y = LabelEncoder()\n",
    "le_y.fit(y_train)\n",
    "pickle.dump(le_y, open('models/encoding_y.pkl', 'wb'))\n",
    "\n",
    "\n",
    "for col in ['education','self_employed']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train[col].unique())\n",
    "    pickle.dump(le, open('models/encoding_' + col + '.pkl', 'wb'))\n",
    "    X_train[col] = le.transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "X_train_df = X_train\n",
    "X_train= X_train.values\n",
    "y_train = le_y.transform(y_train)\n",
    "y_test = le_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Model Variable \n",
    "rf_model = RandomForestClassifier(verbose= False)\n",
    "cat_model = CatBoostClassifier(verbose = False)\n",
    "xgb_model = XGBClassifier(verbosity=0)\n",
    "\n",
    "X_train = X_train_df\n",
    "X_test=X_test\n",
    "#Model Fitting\n",
    "rf_model.fit(X_train, y_train)\n",
    "cat_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [rf_model,cat_model,xgb_model]\n",
    "result = pd.DataFrame(columns=['model_name','accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "for ind, m in enumerate(models):\n",
    "    accuracies = cross_val_score(estimator = m, X = X_train, y = y_train, cv = 3)\n",
    "    if ind!=1:\n",
    "        result.loc[ind,'model_name']= 'model_'+ str(m).split(\"(\")[0]\n",
    "    else:\n",
    "        result.loc[ind,'model_name']='model_'+ str(m).split(\".\")[2].split(\" \")[0]\n",
    "    result.loc[ind,'accuracy']= accuracies.mean()*100\n",
    "\n",
    "print('CROSS-VAL RESULTS')\n",
    "print(result)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('=============================')\n",
    "print('TEST RESULTS')\n",
    "for ind,m in enumerate(models):\n",
    "    y_pred = models[ind].predict(X_test)\n",
    "    ac = accuracy_score(y_test, y_pred)\n",
    "    rocauc = roc_auc_score(y_test, m.predict_proba(X_test)[:, 1])\n",
    "    if ind!=1:\n",
    "        print(str('model_'+ str(m).split(\"(\")[0]) + ' -> Accuracy : ' + str(ac) + ' ROCAUC :' + str(rocauc))\n",
    "    else:\n",
    "        print(str('model_'+ str(m).split(\".\")[2].split(\" \")[0] + ' -> Accuracy : ' + str(ac) + ' ROCAUC :' + str(rocauc)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF PARAMS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [100,200,500,1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [2,4,6,8,10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(verbose= False)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf_model,\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy_rf = grid_search.best_score_\n",
    "best_parameters_rf = grid_search.best_params_\n",
    "print(best_parameters_rf) \n",
    "{'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#XGB PARAMS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'gamma': [0,0.1,0.2,0.4,1.6,10,50,100],\n",
    "              'learning_rate': [0.01,0.05, 0.1,  0.2,0.5, 0.6],\n",
    "              'max_depth': [5,8,10,50,80,100],\n",
    "              'n_estimators': [50,80,100,150,200],\n",
    "              'reg_alpha': [0,0.1,5,20,50],\n",
    "              'reg_lambda': [0,0.1,5,20,50]}\n",
    "\n",
    "xgb_model = XGBClassifier(verbosity=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = xgb_model,\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy_rf = grid_search.best_score_\n",
    "best_parameters_rf = grid_search.best_params_\n",
    "print(best_parameters_rf) \n",
    "#{'max_depth': 80, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.03, 0.1,0.2,0.5],\n",
    "        'depth': [2,4, 6, 10,20],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
    "model = CatBoostClassifier(verbose=False)\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy_rf = grid_search.best_score_\n",
    "best_parameters_rf = grid_search.best_params_\n",
    "print(best_parameters_rf) \n",
    "#{'depth': 10, 'l2_leaf_reg': 7, 'learning_rate': 0.03}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(models[0].predict, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(models[1].predict, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(models[2].predict, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL MODELS\n",
    "# Model imports\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Model Variable \n",
    "rf_model = RandomForestClassifier(criterion= 'entropy', \n",
    "                                  max_depth= 8, \n",
    "                                  max_features='sqrt', \n",
    "                                  n_estimators = 500,\n",
    "                                  verbose= False)\n",
    "cat_model = CatBoostClassifier(depth=10,\n",
    "                                l2_leaf_reg=7, \n",
    "                                learning_rate=0.03,\n",
    "                                verbose = False)\n",
    "xgb_model = XGBClassifier(gamma =0,\n",
    "                          learning_rate= 0.5, \n",
    "                          max_depth = 5 ,\n",
    "                          n_estimators = 80, \n",
    "                          reg_alpha = 0.1, \n",
    "                          reg_lambda = 0.1,\n",
    "                          verbosity = 0)\n",
    "\n",
    "#Model Fitting\n",
    "rf_model.fit(X_train, y_train)\n",
    "cat_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print('TEST RESULTS')\n",
    "for ind,m in enumerate(models):\n",
    "    y_pred = models[ind].predict(X_test)\n",
    "    ac = accuracy_score(y_test, y_pred)\n",
    "    rocauc = roc_auc_score(y_test, m.predict_proba(X_test)[:, 1])\n",
    "    if ind!=1:\n",
    "        print(str('model_'+ str(m).split(\"(\")[0]) + ' -> Accuracy : ' + str(ac) + ' ROCAUC :' + str(rocauc))\n",
    "    else:\n",
    "        print(str('model_'+ str(m).split(\".\")[2].split(\" \")[0] + ' -> Accuracy : ' + str(ac) + ' ROCAUC :' + str(rocauc)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TEST RESULTS\n",
    "model_RandomForestClassifier -> Accuracy : 0.9836065573770492 ROCAUC :0.9968922033584999\n",
    "model_CatBoostClassifier -> Accuracy : 0.9859484777517564 ROCAUC :0.9977413596899134\n",
    "BEST -> model_XGBClassifier -> Accuracy : 0.9882903981264637 ROCAUC :0.9985327502844962\n",
    "'''\n",
    "\n",
    "pickle.dump(xgb_model,open('classification_model.pkl', 'wb')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj",
   "language": "python",
   "name": "hj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
